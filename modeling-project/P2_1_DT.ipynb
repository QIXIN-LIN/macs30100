{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "note: because the pre-processing 90k+ images is time-consuming, I would use a subset I create to complete this assignment. This subset has average distribution label for gender-race combination to avoid potential bias."
      ],
      "metadata": {
        "id": "gNARTFJ_MP8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "6prQqP9qkWuj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the image dataset\n",
        "\n",
        "%cd /content\n",
        "!gdown --id '13shxKy6WSeAa7dPhccnSG9aoFZ76lVPT' --output fairface-img-margin025-trainval.zip\n",
        "!unzip fairface-img-margin025-trainval.zip"
      ],
      "metadata": {
        "id": "6nl6lBY1ibac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load labels\n",
        "file_path = 'fairface_subset.csv'\n",
        "labels_df = pd.read_csv(file_path)\n",
        "\n",
        "labels_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRS2uGMhjtdm",
        "outputId": "59ffe82e-c439-41f7-d47b-b7bf731b16ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6300, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess images\n",
        "def load_and_preprocess_image(image_path):\n",
        "  # Load image\n",
        "  image = Image.open(image_path)\n",
        "  image = image.resize((64, 64))\n",
        "\n",
        "  # Convert to grayscale\n",
        "  image = image.convert('L')\n",
        "\n",
        "  # Convert to numpy array and flatten\n",
        "  image_array = np.array(image).flatten()\n",
        "  return image_array\n",
        "\n",
        "\n",
        "images = [load_and_preprocess_image(f'{fname}') for fname in labels_df['file']]"
      ],
      "metadata": {
        "id": "mpe4JxSakrrF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labelling Gender (major)"
      ],
      "metadata": {
        "id": "m7ufxSg0sMy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix X and labels y\n",
        "X = np.array(images)\n",
        "y = labels_df['gender'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ZJ5Drc8ulhxt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Decision Tree Model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "print(\"Decision Tree Performance: \" + str(np.round(dt_model.score(X_test,y_test),3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwMY8aLNl4nm",
        "outputId": "421c7f7a-5ac8-47a4-fcd3-2758322acf6f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Performance: 0.608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change parameters of Decision Tree Model\n",
        "\n",
        "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42, criterion=\"entropy\")\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluete Dicision Tree\n",
        "print(\"Decision Tree Performance: \" + str(np.round(dt_model.score(X_test,y_test),3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6aXkkdbtsuv",
        "outputId": "f094d368-cdc1-4e29-e83f-a7f296cdde22"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Performance: 0.634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting! By avoiding overfitting, I improve the performance of the model. I thought the image data will always be more complex than text data, but it looks like that the tree don't need to be super deep."
      ],
      "metadata": {
        "id": "GvGAPf1_u_qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "print(\"Random Forest Performance: \" + str(np.round(rf_model.score(X_test,y_test),3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RKAWbHKsi2h",
        "outputId": "d418602c-c411-42c0-9bbc-b8cf7828cbac"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Performance: 0.686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change parameters of Random Forest Model\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, bootstrap=False)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "print(\"Random Forest Performance:\" + str(np.round(rf_model.score(X_test,y_test),3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAV8T3S8vvI0",
        "outputId": "76546eca-b895-42e2-f142-8ce7379e4b99"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Performance:0.703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also interesting! When I set bootstrap as False, the performance of the model is improved. Really a anti-intuition for me."
      ],
      "metadata": {
        "id": "z_tXuNic0i9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labelling Race (minor)"
      ],
      "metadata": {
        "id": "b27zK-Bsscfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix X and labels y\n",
        "X = np.array(images)\n",
        "y = labels_df['race'].values  # Replace 'label' with your actual label column name\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zlKMwkWMmyEX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Decision Tree Model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "print(\"Decision Tree Performance:\")\n",
        "print(classification_report(y_test, dt_predictions))\n",
        "\n",
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "print(\"Random Forest Performance:\")\n",
        "print(classification_report(y_test, rf_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjpAxUVLmyev",
        "outputId": "41a965d6-2a81-4c81-f540-398a9a516981"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Performance:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "          Black       0.26      0.27      0.27       180\n",
            "     East Asian       0.30      0.25      0.27       201\n",
            "         Indian       0.20      0.22      0.21       166\n",
            "Latino_Hispanic       0.15      0.16      0.15       179\n",
            " Middle Eastern       0.12      0.14      0.13       163\n",
            "Southeast Asian       0.22      0.19      0.20       189\n",
            "          White       0.21      0.20      0.20       182\n",
            "\n",
            "       accuracy                           0.20      1260\n",
            "      macro avg       0.21      0.20      0.20      1260\n",
            "   weighted avg       0.21      0.20      0.21      1260\n",
            "\n",
            "Random Forest Performance:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "          Black       0.42      0.57      0.48       180\n",
            "     East Asian       0.38      0.39      0.39       201\n",
            "         Indian       0.31      0.37      0.34       166\n",
            "Latino_Hispanic       0.23      0.18      0.20       179\n",
            " Middle Eastern       0.20      0.20      0.20       163\n",
            "Southeast Asian       0.28      0.19      0.23       189\n",
            "          White       0.26      0.27      0.27       182\n",
            "\n",
            "       accuracy                           0.31      1260\n",
            "      macro avg       0.30      0.31      0.30      1260\n",
            "   weighted avg       0.30      0.31      0.30      1260\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some thoughts"
      ],
      "metadata": {
        "id": "t1Ham84604wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance and Overfitting\n",
        "+ The Random Forest model showed better results compared to the Decision Tree model. This aligns with expectations, as Random Forests usually offer improved generalization by combining multiple trees. A key observation was that preventing overfitting significantly improved performance, indicating that even for image data, which is often perceived as more complex than text data, a highly complex model (like a very deep tree) is not always necessary. This finding challenges the common assumption that image data invariably requires more complex models.\n",
        "\n",
        "### Impact of Bootstrap Parameter\n",
        "+ Interestingly, setting bootstrap to False improved the Random Forest's performance. Typically, bootstrap sampling in Random Forests introduces diversity, enhancing robustness. However, in this case, allowing each tree to train on the full dataset may have reduced bias, leading to better results. This suggests that for certain datasets, the usual advantage of bootstrapping might be less impactful.\n",
        "\n",
        "### Efficiency\n",
        "+ Decision Trees were more computationally efficient, highlighting their suitability for smaller datasets or scenarios where rapid training is crucial. Random Forests, involving multiple trees, are more computationally demanding, which becomes a significant factor with larger datasets.\n",
        "\n",
        "### Pros and Cons\n",
        "+ Decision Trees:\n",
        " + Pros: Simple, interpretable, faster training.\n",
        " + Cons: Prone to overfitting, less effective with complex patterns.\n",
        "+ Random Forests:\n",
        " + Pros: Better accuracy, handles overfitting well, good for complex data.\n",
        " + Cons: Computationally intensive, less interpretable.\n"
      ],
      "metadata": {
        "id": "aZJ44t7_0_Sj"
      }
    }
  ]
}